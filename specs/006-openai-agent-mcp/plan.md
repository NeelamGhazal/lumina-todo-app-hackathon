# Implementation Plan: OpenAI Agent with MCP Integration

**Branch**: `006-openai-agent-mcp` | **Date**: 2026-02-08 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/specs/006-openai-agent-mcp/spec.md`

## Summary

Implement a conversational AI agent using OpenAI Python SDK routed through OpenRouter. The agent understands natural language task requests, calls MCP tools from Part 1 to perform operations, and returns friendly conversational responses. Key features: manual tool execution pattern, database-stored conversation context (last 10 messages), 30-minute session timeout, and user-friendly error handling.

## Technical Context

**Language/Version**: Python 3.13+
**Primary Dependencies**: openai (Python SDK), httpx, fastapi, sqlmodel, pydantic, structlog
**Storage**: Neon PostgreSQL (shared with Part 1 - conversations, messages tables)
**Testing**: pytest, pytest-asyncio, pytest-cov, httpx (test client)
**Target Platform**: Linux server (Docker-ready)
**Project Type**: Backend service (extends chatbot/ from Part 1)
**Performance Goals**: <3 seconds response time (SC-002), LLM latency included
**Constraints**: OpenRouter API only (no direct OpenAI), cost-optimized (gpt-4o-mini default)
**Scale/Scope**: Single user concurrency per request, 10-message context window

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

| Principle | Status | Notes |
|-----------|--------|-------|
| I. Spec-Driven Development | PASS | Spec complete with 6 user stories, 25 FRs |
| II. Professional Quality | PASS | Type hints, Pydantic models, async, tests planned |
| III. Visual Excellence | N/A | Backend service - no UI |
| IV. Task-Driven Implementation | PENDING | Tasks.md to be generated by /sp.tasks |
| V. Checkpoint Control | PASS | 4 checkpoints defined |
| VI. AI-First Engineering | PASS | OpenAI Agents SDK via OpenRouter |
| VII. Cloud-Native Mindset | PASS | Extends existing Docker setup from Part 1 |

**Gate Status: PASS** - All applicable principles satisfied.

## Project Structure

### Documentation (this feature)

```text
specs/006-openai-agent-mcp/
├── plan.md              # This file
├── research.md          # Phase 0 output
├── data-model.md        # Phase 1 output
├── quickstart.md        # Phase 1 output
├── contracts/           # Phase 1 output
│   └── agent-api.yaml   # Chat API contract
└── tasks.md             # Phase 2 output (via /sp.tasks)
```

### Source Code (repository root)

```text
chatbot/
├── mcp_server/          # From Part 1 (MCP tools)
│   ├── main.py          # FastAPI app - ADD /chat, /conversations endpoints
│   ├── models.py        # Conversation, Message (existing)
│   ├── schemas.py       # Tool schemas (existing)
│   └── ...
├── agent/               # NEW - Agent implementation
│   ├── __init__.py
│   ├── config.py        # Agent config, instructions, model settings
│   ├── client.py        # OpenRouter client wrapper
│   ├── tools.py         # MCP tool integration layer
│   ├── conversation.py  # Conversation/session manager
│   ├── chat.py          # Chat orchestration (main logic)
│   └── schemas.py       # Chat request/response models
└── tests/
    ├── test_agent_client.py      # OpenRouter client tests
    ├── test_agent_tools.py       # Tool conversion/execution tests
    ├── test_agent_conversation.py # Session management tests
    ├── test_agent_chat.py        # Chat orchestration tests
    └── test_chat_integration.py  # End-to-end tests
```

**Structure Decision**: Extends existing `chatbot/` project with new `agent/` module. Reuses Part 1's FastAPI app and database setup.

## Architecture Decision Records (ADRs)

### ADR-005: Manual Tool Execution Pattern

**Context**: How should the agent execute MCP tools when the LLM requests them?

**Decision**: Use manual tool execution pattern where system receives tool calls from LLM, executes them against MCP server, and submits results back.

**Options Considered**:
1. **Auto-execute** - LLM auto-calls tools (not supported by OpenRouter)
2. **Manual execution** - System handles tool calls explicitly ✓
3. **Hybrid** - Auto for simple, manual for complex (over-engineered)

**Rationale**:
- Full control over tool execution and error handling
- Ability to inject user_id and validate parameters
- Better logging and observability
- Works with OpenRouter's Chat Completions API

**Trade-offs**: More code than auto-magic, but necessary for OpenRouter compatibility.

---

### ADR-006: Database-Only Context Storage

**Context**: Where to store conversation history for multi-turn context?

**Decision**: Store all messages in PostgreSQL database. Retrieve last 10 messages and send with each LLM request.

**Options Considered**:
1. **OpenAI Assistants API threads** - Not supported by OpenRouter
2. **Database only** - Full control, portable, works with OpenRouter ✓
3. **In-memory cache** - Lost on restart, not scalable

**Rationale**:
- OpenRouter only supports Chat Completions (no Assistants API)
- Database storage enables conversation history viewing (FR-054)
- Aligns with Part 1's existing Conversation/Message schema
- 10 messages covers SC-007 (5 exchanges) with margin

**Trade-offs**: Database query per request, but SQLModel with async is fast.

---

### ADR-007: Startup Tool Schema Loading

**Context**: When should the system fetch MCP tool schemas?

**Decision**: Fetch MCP tool schemas once at startup and convert to OpenAI function format.

**Options Considered**:
1. **Startup only** - Simple, fast, tools are stable ✓
2. **Per-request** - Fresh but wasteful
3. **Cached with TTL** - Over-engineered for stable schemas

**Rationale**:
- MCP tool schemas from Part 1 don't change at runtime
- Single HTTP call on startup vs. per-request overhead
- Startup failure = fast fail (health check fails)

**Trade-offs**: Must restart to pick up tool changes (acceptable for development).

---

### ADR-008: LLM-Based Intent Extraction

**Context**: How should the agent understand user intent?

**Decision**: Rely on LLM (via OpenRouter) for intent recognition and parameter extraction.

**Options Considered**:
1. **LLM-based** - High accuracy, low maintenance ✓
2. **Rule-based regex** - Brittle, lots of edge cases
3. **Hybrid** - Unnecessary complexity

**Rationale**:
- LLM (gpt-4o-mini) excels at intent recognition
- Tool descriptions guide appropriate usage
- No custom NLU model to maintain
- Handles variations naturally ("add task", "remind me", "new todo")

**Trade-offs**: API cost per request, but gpt-4o-mini is cheap.

---

### ADR-009: Immediate Error Response (No Retry)

**Context**: How to handle MCP tool failures?

**Decision**: Return immediate user-friendly error message. No automatic retry.

**Options Considered**:
1. **Immediate error** - Simple, predictable ✓
2. **Retry once** - Adds latency, complexity
3. **Exponential backoff** - Over-engineered for user-facing

**Rationale**:
- Per clarification: User prefers immediate feedback
- Users can manually retry if needed
- Prevents cascading timeouts
- Keeps response time predictable

**Trade-offs**: User must retry manually, but that's expected UX.

## Implementation Phases

### Phase 2A: OpenRouter Client Setup (45-60 mins)

**Purpose**: Initialize OpenAI SDK with OpenRouter and verify connectivity.

**Objective**: Agent can send messages to OpenRouter and receive responses.

**Components**:
- `chatbot/agent/config.py` - Configuration and agent instructions
- `chatbot/agent/client.py` - OpenRouter client wrapper
- `chatbot/agent/schemas.py` - Chat request/response models

**Checkpoint 1**: Client initialized, basic message send/receive works via curl test.

---

### Phase 2B: MCP Tool Integration (1-1.5 hours)

**Purpose**: Fetch MCP tools and integrate with OpenAI function calling.

**Objective**: Agent can call MCP tools and receive results.

**Components**:
- `chatbot/agent/tools.py` - MCP schema conversion, tool execution
- Update `chatbot/agent/client.py` - Add tool definitions to LLM calls

**Checkpoint 2**: All 5 MCP tools registered, tool execution works, errors handled.

---

### Phase 2C: Conversation Management (1-1.5 hours)

**Purpose**: Implement session management and context retrieval.

**Objective**: Multi-turn conversations maintain context correctly.

**Components**:
- `chatbot/agent/conversation.py` - Session lifecycle, message storage
- Update `chatbot/mcp_server/main.py` - Add /chat endpoint
- Add /conversations endpoints

**Checkpoint 3**: Conversations persist, context maintained across turns, 30-min timeout works.

---

### Phase 2D: Response Formatting & Testing (1.5-2 hours)

**Purpose**: Complete integration with natural responses and full test coverage.

**Objective**: All tests pass with 80%+ coverage.

**Components**:
- `chatbot/agent/chat.py` - Main orchestration logic
- Response formatters for each tool result
- Unit tests for all agent components
- Integration tests with MCP server

**Checkpoint 4**: All tests passing, 80%+ coverage, natural responses verified.

## Dependencies & Execution Order

```
Phase 2A (OpenRouter Client)
    │
    ▼
Phase 2B (MCP Tool Integration) ← Requires Part 1 MCP server
    │
    ▼
Phase 2C (Conversation Management)
    │
    ▼
Phase 2D (Testing & Formatting)
```

## Component Architecture

```
┌────────────────────────────────────────────────────────────┐
│                     FastAPI Application                     │
│                    (mcp_server/main.py)                    │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  Existing (Part 1)          New (Part 2)                  │
│  ┌──────────────┐          ┌──────────────┐               │
│  │ /mcp/tools   │          │ /chat        │               │
│  │ /mcp/call    │          │ /conversations│               │
│  │ /health      │          │              │               │
│  └──────────────┘          └──────┬───────┘               │
│                                   │                        │
└───────────────────────────────────┼────────────────────────┘
                                    │
                    ┌───────────────┼───────────────┐
                    │               │               │
                    ▼               ▼               ▼
            ┌───────────┐   ┌───────────┐   ┌───────────┐
            │  chat.py  │───│ tools.py  │───│ client.py │
            │           │   │           │   │           │
            │Orchestrate│   │MCP Bridge │   │ OpenRouter│
            └─────┬─────┘   └─────┬─────┘   └───────────┘
                  │               │
                  ▼               ▼
            ┌───────────┐   ┌───────────┐
            │conversation│   │MCP Server │
            │   .py     │   │(Part 1)   │
            │           │   │localhost  │
            │Session Mgr│   │  :8001    │
            └─────┬─────┘   └───────────┘
                  │
                  ▼
            ┌───────────┐
            │ Database  │
            │ Postgres  │
            │conversations│
            │ messages  │
            └───────────┘
```

## Testing Strategy

### Unit Tests (per component)
- `test_agent_client.py` - OpenRouter initialization, message sending, error handling
- `test_agent_tools.py` - Schema conversion, tool execution, parameter injection
- `test_agent_conversation.py` - Session creation, timeout logic, message retrieval
- `test_agent_chat.py` - Orchestration, tool loop handling, response formatting

### Integration Tests
- `test_chat_integration.py` - Full /chat endpoint with real MCP server
- Multi-turn conversation flows
- Error scenarios (MCP down, invalid input)

### Coverage Target
- 80%+ overall (aligns with SC-008)
- 90%+ for chat.py (critical path)

## Success Criteria per Checkpoint

### Checkpoint 1 (Phase 2A)
- [ ] OpenRouter client initializes without error
- [ ] Basic message returns response
- [ ] Environment variables load correctly
- [ ] Health check includes agent status

### Checkpoint 2 (Phase 2B)
- [ ] All 5 MCP tools appear as OpenAI functions
- [ ] Tool schema conversion is correct
- [ ] Tool execution returns valid results
- [ ] Error handling returns user-friendly messages

### Checkpoint 3 (Phase 2C)
- [ ] New conversation created on first message
- [ ] Existing conversation reused within 30 minutes
- [ ] New conversation after 30-minute gap
- [ ] Last 10 messages included in context
- [ ] /conversations endpoint lists user's conversations
- [ ] /conversations/{id}/messages returns history

### Checkpoint 4 (Phase 2D)
- [ ] All unit tests passing
- [ ] All integration tests passing
- [ ] Coverage ≥ 80%
- [ ] Natural language responses verified
- [ ] Response time < 3 seconds (excluding outliers)
- [ ] README updated with agent setup

## Risk Mitigation

| Risk | Mitigation |
|------|------------|
| OpenRouter rate limits | Use gpt-4o-mini (higher limits), add rate limit error handling |
| MCP server unavailable | Fast fail with user-friendly message, health check dependency |
| Context window overflow | Fixed 10-message limit, token count not needed |
| Infinite tool loops | Max 5 tool call rounds per request |
| Database connection issues | Reuse Part 1's connection pool, proper error handling |

## Next Steps

1. Run `/sp.tasks` to generate detailed task list
2. Execute Phase 2A tasks
3. Checkpoint 1 review
4. Continue through phases 2B-2D
5. Final review at Checkpoint 4

---

*Plan generated: 2026-02-08*
*Ready for: /sp.tasks*
